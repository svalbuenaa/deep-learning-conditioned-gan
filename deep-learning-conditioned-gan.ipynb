{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Conditional GANs\n\nGenerative Adversarial Networks (GANs) allow us to generate image, video, or audio data from a random input. Typically, the random input is sampled from a normal distribution before undergoing a series of transformations that convert it into something plausible (image, video, audio, etc.).\n\nHowever, a simple DCGAN does not allow us to control the appearance (e.g., the class) of the samples we are generating. For instance, with a GAN that generates handwritten digits, a simple DCGAN would not let us choose the class of the digits being generated. To control what we generate, we need to condition the GAN's output on a semantic input, such as the class of an image.\n\nHere, we will build a Conditional GAN that can generate handwritten digits conditioned on a specific class.\n\nThe applications of GANs are highly varied, given the wide range of data types they can handle. Essentially, in a GAN, all we need is a \"real\" dataset and the ability to approximate a function that allows us to generate new instances (via the generator network) capable of \"fooling\" a network that tries to distinguish between real and fake instances (the discriminator network). On this basis, for a wide variety of data types, GANs can help us generate sufficiently similar instances of those data types.\n\nHere are some applications of GANs:\n\n1. Image Generation: Imagine we have a dataset of images we want to use for training a model, but the number of available images is limited. We can augment the dataset using GANs to create new instances for each class in the original dataset.\n\n2. Image Upscaling: GANs can be trained to enhance the resolution of an image. For this, we would need a dataset of paired low-resolution and high-resolution images to train the GAN.\n\"Vector Operations\": Similar to how word representations in transformers involve vectors with semantically meaningful dimensions (e.g., the concept of \"queen\" derived from the vectors for \"king,\" \"man,\" and \"woman\"), GANs can be used to generate images that result from \"operations\" on other images. For example, aging a face.\n\n3. Text-to-Image: As another variant of image-related applications, GANs (like GigaGAN) can be used to generate images from text descriptions. However, models based on diffusion, such as those powering MidJourney or DALL-E, are often preferred for this task.\n\n4. Other Data Types: These examples are just a glimpse of the data types that can be generated using GANs. In principle, GANs can also generate other types of data, such as videos, audio, or text—and even beyond these. Many machine learning models require large volumes of data to generalize well (a concept we've seen repeatedly in this course). GANs can help generate new data instances to feed these models. Here are a few examples:\n\n5. Fraud Detection: The goal of fraud detection in, for example, online transactions, is to have as few fraudulent transactions as possible. This scarcity makes training models to detect fraud challenging. If we have 10 million transactions for training and only 0.01% are fraudulent, the class imbalance is too significant, making it difficult to avoid overfitting. GANs can be used to increase the amount of fraudulent data, thereby addressing class imbalance in the dataset used to train the fraud detection model.\n\n6. Anomaly Detection in Industry: Detecting defective parts can be crucial in industries (e.g., for safety reasons). Just as it can be difficult to obtain fraudulent instances, acquiring anomalous instances in industrial settings can also be challenging. To train better anomaly detection models, GANs can be used to augment the data with anomalous instances.\n\nAs we can see, most applications of GANs revolve around data augmentation.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install -q git+https://github.com/tensorflow/docs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T11:34:09.176095Z","iopub.execute_input":"2024-12-25T11:34:09.176558Z","iopub.status.idle":"2024-12-25T11:34:21.916547Z","shell.execute_reply.started":"2024-12-25T11:34:09.176513Z","shell.execute_reply":"2024-12-25T11:34:21.915347Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## 0. Libraries","metadata":{}},{"cell_type":"code","source":"import keras\n\nfrom keras import layers\nfrom keras import ops\nfrom tensorflow_docs.vis import embed\nimport tensorflow as tf\nimport numpy as np\nimport imageio\nfrom sklearn.preprocessing import OneHotEncoder\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T11:34:33.213169Z","iopub.execute_input":"2024-12-25T11:34:33.213543Z","iopub.status.idle":"2024-12-25T11:34:43.733967Z","shell.execute_reply.started":"2024-12-25T11:34:33.213515Z","shell.execute_reply":"2024-12-25T11:34:43.732827Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## 0.1. Definition of constants and hyperparameters","metadata":{}},{"cell_type":"code","source":"batch_size = 64\nnum_channels = 1\nnum_classes = 10\nimage_size = 28\nlatent_dim = 128","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T11:35:03.352974Z","iopub.execute_input":"2024-12-25T11:35:03.353670Z","iopub.status.idle":"2024-12-25T11:35:03.358518Z","shell.execute_reply.started":"2024-12-25T11:35:03.353625Z","shell.execute_reply":"2024-12-25T11:35:03.357428Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"**About latent_dim**: We can understand images in terms of multidimensional vectors. However, we can also represent them in a space with fewer dimensions, known as the latent space. Each point in the latent space maps to a point in the image's dimensional space. Thus, by providing the generator with a point (i.e., a vector) from the latent space, the generator can produce an image. The value of latent_dim is simply the number of dimensions in the latent space—in our case, 128. This 128-dimensional space is much simpler than the high-dimensional space required to represent each \"complete\" image.\n\n\n\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Dataset loading and preprocessing\n\nWe will use the dataset from: https://www.kaggle.com/datasets/jordidelatorreuoc/handwritten-digits-with-writer-characteristics/data\n\nWe will fetch the 28x28 version, analyze the contents of the dataset, identify the variables of interest (images in 28x28 format and labels), and load them into a TensorFlow dataset. The variables must be loaded and normalized to the range [0,1]. The classes should be one-hot encoded, e.g., class 9 as [0,0,0,0,0,0,0,0,1], and the images must have the correct dimensions to be processed later with TensorFlow.\n\nThe expected output dimensions are:\n\nShape of images: (13580, 28, 28, 1)\nShape of labels: (13580, 10)\nThe output of the process should be a dataset variable of type tf.data.Dataset.\n\nThe dataset contains six files, three of which provide information about the dataset. The file `Images(28x28).npy` contains the images in 28x28 pixel format, while the file `Images(500x500).npy` contains the images in 500x500 pixel format. Finally, the file `WriterInfo.npy` contains the labels (in the first column) as well as information about the person who wrote each number (which is actually the unique aspect of this dataset).","metadata":{}},{"cell_type":"code","source":"# Load the images, normalize the values to be between 0 and 1, and adjust the shape to the desired output.\nimages = np.load('/kaggle/input/handwritten-digits-with-writer-characteristics/HDW+/Images(28x28).npy')\nimages = images/255\nimages = images.reshape(13580, 28, 28, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T12:15:18.021747Z","iopub.execute_input":"2024-12-25T12:15:18.022110Z","iopub.status.idle":"2024-12-25T12:15:18.475842Z","shell.execute_reply.started":"2024-12-25T12:15:18.022082Z","shell.execute_reply":"2024-12-25T12:15:18.474823Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Load labels\nlabels = np.load('/kaggle/input/handwritten-digits-with-writer-characteristics/HDW+/WriterInfo.npy')[:,0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T12:15:35.610379Z","iopub.execute_input":"2024-12-25T12:15:35.610802Z","iopub.status.idle":"2024-12-25T12:15:35.623890Z","shell.execute_reply.started":"2024-12-25T12:15:35.610761Z","shell.execute_reply":"2024-12-25T12:15:35.622859Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Show the number of instances per digit\nunique, counts = np.unique(labels, return_counts=True)\nprint(np.asarray((unique, counts)).T)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T12:15:54.253381Z","iopub.execute_input":"2024-12-25T12:15:54.253764Z","iopub.status.idle":"2024-12-25T12:15:54.261288Z","shell.execute_reply.started":"2024-12-25T12:15:54.253734Z","shell.execute_reply":"2024-12-25T12:15:54.260400Z"}},"outputs":[{"name":"stdout","text":"[[   0 1358]\n [   1 1358]\n [   2 1358]\n [   3 1358]\n [   4 1358]\n [   5 1358]\n [   6 1358]\n [   7 1358]\n [   8 1358]\n [   9 1358]]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# OneHot encoding\nohe = OneHotEncoder(sparse_output=False)\nlabels_onehot = ohe.fit_transform(labels.reshape(-1, 1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T12:16:08.226012Z","iopub.execute_input":"2024-12-25T12:16:08.226420Z","iopub.status.idle":"2024-12-25T12:16:08.239868Z","shell.execute_reply.started":"2024-12-25T12:16:08.226385Z","shell.execute_reply":"2024-12-25T12:16:08.238742Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Show an example\nprint(labels[1000])\nprint(labels_onehot[1000])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T12:16:19.069892Z","iopub.execute_input":"2024-12-25T12:16:19.070214Z","iopub.status.idle":"2024-12-25T12:16:19.076243Z","shell.execute_reply.started":"2024-12-25T12:16:19.070189Z","shell.execute_reply":"2024-12-25T12:16:19.075383Z"}},"outputs":[{"name":"stdout","text":"2\n[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"temp = images.reshape(13580, 28, 28, 1)\nprint(f\"Shape of images: {images.shape}\")\nprint(f\"Shape of labels: {labels_onehot.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T12:16:41.429386Z","iopub.execute_input":"2024-12-25T12:16:41.429764Z","iopub.status.idle":"2024-12-25T12:16:41.434631Z","shell.execute_reply.started":"2024-12-25T12:16:41.429736Z","shell.execute_reply":"2024-12-25T12:16:41.433583Z"}},"outputs":[{"name":"stdout","text":"Shape of images: (13580, 28, 28, 1)\nShape of labels: (13580, 10)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"dataset = tf.data.Dataset.from_tensor_slices((images, labels_onehot))\ndataset = dataset.batch(batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T12:16:51.860478Z","iopub.execute_input":"2024-12-25T12:16:51.860828Z","iopub.status.idle":"2024-12-25T12:16:52.036708Z","shell.execute_reply.started":"2024-12-25T12:16:51.860800Z","shell.execute_reply":"2024-12-25T12:16:52.035656Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## 2. Number of input channels for the generator and the discriminator\n\nIn a regular (non-conditional) GAN, we start by sampling noise (of a fixed dimension) from a normal distribution. In our case, we also need to account for class labels. We will need to add the class labels to the input channels of the generator (noise input) and also to the discriminator (generated image input).","metadata":{}},{"cell_type":"code","source":"generator_in_channels = latent_dim + num_classes\ndiscriminator_in_channels = num_channels + num_classes\nprint(generator_in_channels, discriminator_in_channels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T12:18:02.757242Z","iopub.execute_input":"2024-12-25T12:18:02.757673Z","iopub.status.idle":"2024-12-25T12:18:02.763239Z","shell.execute_reply.started":"2024-12-25T12:18:02.757643Z","shell.execute_reply":"2024-12-25T12:18:02.762265Z"}},"outputs":[{"name":"stdout","text":"138 11\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"`generator_in_channels`: Specifies the number of dimensions the generator's input will have. This should correspond to the dimensions of the latent space (from which random vectors are drawn to feed the generator) plus the dimensions of the vector indicating the class to which each image belongs.\n\n`discriminator_in_channels`: Specifies the number of dimensions the discriminator's input will have. This should correspond to the number of channels in each image (in our case, only one, since these are grayscale images), plus, again, the dimensions of the vector indicating the class to which each image belongs. This is because it’s not enough for the generator to simply learn to create coherent numbers; since this is a conditional GAN, we want it to generate the specific number it is supposed to generate, and this must also be trained with the help of the discriminator.","metadata":{}}]}